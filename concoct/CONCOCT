#!/usr/bin/env python
from __future__ import division

import sys
import os
import re

import pandas as p
import numpy as np

from itertools import tee, izip, product
from argparse import ArgumentParser,ArgumentTypeError
from datetime import datetime

from Bio import SeqIO

from sklearn.preprocessing import scale
from sklearn.mixture import GMM
from sklearn.decomposition import PCA

import multiprocessing

from concoct.output import Output

def parallelized_cluster(args):
    c, cv_type,inits,iters,transform_filter = args
    #Run GMM on the pca transform of contigs with kmer count greater
    #than threshold
    gmm = GMM(n_components=c, covariance_type=cv_type, n_init=inits,
              n_iter=iters).fit(transform_filter)
    print >> sys.stderr, "Convergence for cluster number {0}: {1}".format(
        c,gmm.converged_)
    bic = gmm.bic(transform_filter)
    return bic,c


def main(args):
    Output(args.basename,args)
    composition,threshold_filter = composition_data(args.kmer_length,args.composition_file,args.limit_kmer_count)
    cov,cov_range = coverage_data(args.coverage_file,args.coverage_file_column_names)
    joined,pca,transform_filter = join_data(composition,cov,threshold_filter,args.limit_kmer_count,cov_range,args.split_pca)
    cluster(joined,threshold_filter,args.limit_kmer_count,args.clusters,args.executions,args.iterations,transform_filter,args.max_n_processors,pca,args.pipe)


def cluster(joined,threshold_filter,threshold,clusters_range,inits,iters,transform_filter,max_n_processors,pca,pipe):
    cv_type='full'
    cluster_args = []
    for c in clusters_range:
        cluster_args.append((c,cv_type,inits,iters,transform_filter))
    n_processes = min(multiprocessing.cpu_count(),max_n_processors)
    pool = multiprocessing.Pool(processes=n_processes)
    bics = pool.map(parallelized_cluster,cluster_args)

    Output.write_bic(bics)

    min_bic,optimal_c = min(bics,key=lambda x: x[0])
    gmm = GMM(n_components=optimal_c,covariance_type=cv_type,n_init=inits,
              n_iter=iters).fit(transform_filter)
    
    joined["clustering"] = gmm.predict(pca.transform(joined))
    Output.write_clustering(joined,threshold_filter,threshold,c,pipe)
    Output.write_cluster_means(pca.inverse_transform(gmm.means_),
                                   threshold,c)
    # Covariance matrix is three dimensional if full
    if cv_type == 'full':
        for i,v in enumerate(gmm.covars_):
            Output.write_cluster_variance(pca.inverse_transform(v),
                                          threshold,i)
    else:
        Output.write_cluster_variance(pca.inverse_transform(v),
                                      threshold,1)
        
    pp = gmm.predict_proba(transform_filter)
    Output.write_cluster_responsibilities(
        pp,
        threshold,c)

def join_data(composition,cov,threshold_filter,threshold,cov_range,split_pca):
    if split_pca:
        raise NotImplementedError("Not implemented yet to run seperate PCA")
    else:
        joined = composition.join(
            cov.ix[:,cov_range[0]:cov_range[1]],how="inner")
        joined.ix[:,:] = scale(joined)
        #PCA on the contigs that have kmer count greater than threshold
        Output.write_original_data(joined[threshold_filter],threshold)
        pca = PCA(n_components=0.9).fit(joined[threshold_filter])
        transform_filter = pca.transform(joined[threshold_filter])
        Output.write_pca(transform_filter,
                         threshold,cov[threshold_filter].index)
    return joined,pca,transform_filter

def coverage_data(cov_file,cov_range):
    #Coverage import, file has header and contig ids as index
    #Assume datafile is in log coverage format with pseudo counts
    cov = p.read_table(cov_file,header=0,index_col=0)
    if cov_range is None:
        cov_range = (cov.columns[0],cov.columns[-1])

    ###TODO: Here we expect the data to be coverage not read counts. 
    ###The commented section calculates log coverage from read counts.
    ###log(q_ij) = log[(Y_ij + 1).R/L_i]) where L_i is the length of 
    ###contig i and R is the read length.
    ###cov.ix[:,cov_range[0]:cov_range[1]] = np.log((
    ###       cov.ix[:,cov_range[0]:cov_range[1]]+1
    ###       ).mul(read_length/cov.length))
    cov.ix[:,cov_range[0]:cov_range[1]] = np.log((
        cov.ix[:,cov_range[0]:cov_range[1]] + 0.01))

    cov.ix[:,cov_range[0]:cov_range[1]] = scale(cov.ix[:,cov_range[0]:cov_range[1]])
    return cov,cov_range

def composition_data(kmer_len,comp_file,threshold):
    #Composition
    #Generate kmer dictionary
    feature_mapping, nr_features = generate_feature_mapping(kmer_len)
    #Count lines in composition file
    count_re = re.compile("^>")
    seq_count = 0
    with open(comp_file) as fh:
        for line in fh:
            if re.match(count_re,line):
                seq_count += 1

    #Initialize with ones since we do pseudo count, we have i contigs as rows
    #and j features as columns
    composition = np.ones((seq_count,nr_features))
    
    
    contigs_id = []
    for i,seq in enumerate(SeqIO.parse(comp_file,"fasta")):
        contigs_id.append(seq.id)
        for kmer_tuple in window(seq.seq.tostring().upper(),kmer_len):
            composition[i,feature_mapping["".join(kmer_tuple)]] += 1
    composition = p.DataFrame(composition,index=contigs_id,dtype=float)
    #Select contigs to cluster on
    threshold_filter = composition.sum(axis=1) > threshold
    
    #log(p_ij) = log[(X_ij +1) / rowSum(X_ij+1)]
    composition = np.log(composition.divide(composition.sum(axis=1),axis=0))
    return composition,threshold_filter


def window(seq,n):
    els = tee(seq,n)
    for i,el in enumerate(els):
        for _ in xrange(i):
            next(el, None)
    return izip(*els)

def generate_feature_mapping(kmer_len):
    BASE_COMPLEMENT = {"A":"T","T":"A","G":"C","C":"G"}
    kmer_hash = {}
    counter = 0
    for kmer in product("ATGC",repeat=kmer_len):
        kmer = ''.join(kmer)
        if kmer not in kmer_hash:
            kmer_hash[kmer] = counter
            rev_compl = ''.join([BASE_COMPLEMENT[x] for x in reversed(kmer)])
            kmer_hash[rev_compl] = counter
            counter += 1
    return kmer_hash, counter+1

def parse_cluster_list(cc_string):
    ERROR="'" + cc_string + ("' is not a valid range of number. Expected "
                             "forms like '20,100,2'.")
    try:
        first, last, step = map(int,cc_string.split(","))
    except ValueError as e:
        raise ArgumentTypeError(ERROR)
    except Exception as e:
        raise ArgumentTypeError(ERROR)
    return xrange(first, last+1, step)

def parse_coverage_columns(cov_string):
    ERROR="'" + cov_string + ("' is not valid. Expected 'first_column_name,"
                              "last_column_name'.")
    try:
        cov = cov_string.split(",")
    except ValueError as e:
        raise ArgumentTypeError(ERROR)
    if not len(cov) == 2:
        raise ArgumentTypeError(ERROR)
    return cov

def parse_taxonomy_cluster_list(tax_file):
    raise NotImplementedError(("This functionality has not been added yet. "
                               "Please use -c and specify range"))

def arguments():
    parser = ArgumentParser()

    #Input files
    parser.add_argument('coverage_file',
        help='specify the coverage file')
    parser.add_argument('composition_file',
        help='specify the composition file')

    #Handle cluster number parsing
    cluster_count = parser.add_mutually_exclusive_group()
    cluster_count.add_argument('-c', '--clusters', default=range(20,101,2), 
                               type=parse_cluster_list,
                               help=('specify range of clusters to try out'
                                     ' on format first,last,step.'
                                     ' default 20,100,2.'))
    #Columns in coverage file to use
    parser.add_argument('-n','--coverage_file_column_names', 
                        type=parse_coverage_columns, default=None,
                        help=('specify the first and last column names for'
                              ' continuous coverage range of read counts'
                              ' as first,last'))
    #cluster_count.add_argument('-t', type=parse_taxonomy_cluster_list,
    #help='specify a taxonomy file to estimate species number from (X). \
    #      Will use range X*0.5,X*1.5,2')



    #Kmer length, kmer count threshold and read length
    parser.add_argument('-k','--kmer_length', type=int, default=4,
        help='specify kmer length, defaults to tetramer')
    parser.add_argument('-l','--limit_kmer_count', type=int, default=1000,
        help='specify the kmer count for threshold in running PCA on \
              composition contigs, default 1000')
    parser.add_argument('-r','--read_length', type=int, default=100,
        help='specify read length for coverage, default 100')
    #Joined PCA or seperate PCA
    parser.add_argument('-s','--split_pca', default=False, action="store_true",
        help='specify this flag to first do PCA for the composition \
              and using that component number that explaines 90 percent \
              of variance for the coverage as well. Default join composition \
              and coverage before PCA. ***NOT IMPLEMENTED***')
    #Clustering Parameters
    parser.add_argument('-e', '--executions',type=int, default=5,
        help='How often to initialize each cluster count. default 5 times')
    parser.add_argument('-i', '--iterations',type=int, default=100,
        help='Maximum number of iterations if convergance not achieved')
    #Output
    parser.add_argument('-b', '--basename', default=os.curdir,
        help=("Specify the basename for files or directory where output"
              "will be placed. Path to existing directory or basename"
              "with a trailing '/' will be interpreted as a directory."
              "If not provided, current directory will be used."))
    parser.add_argument('-p', '--pipe', default=False, action="store_true",
                        help=('Add this tag if the main result file should be'
                              'printed to stdout. Useful for pipeline use'))
    parser.add_argument('-m','--max_n_processors',type=int,
                       help='Specify the maximum number of processors CONCOCT is allowed to use, if absent, all present processors will be used.')
    
    return parser.parse_args()

        
if __name__=="__main__":
    args = arguments()
    results = main(args)
